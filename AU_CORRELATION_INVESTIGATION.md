# AU Correlation Gap Investigation

## VERIFIED Findings Only - Last Updated: 2024-12-05

---

## Status Summary

| Metric | Value | Status |
|--------|-------|--------|
| **Landmark Mean Error (vs C++)** | **1.34 px** ‚Üí **2.24 px** | ‚ö†Ô∏è NEEDS IMPROVEMENT |
| Overall AU Correlation (Python Pipeline) | **~69%** | IMPROVED |
| Upper Face AU Correlation | **99%** | EXCELLENT |
| Lower Face AU Correlation | **55-70%** | NEEDS WORK |
| **HOG Feature Correlation (same image)** | **100%** | **VERIFIED CORRECT** |
| **Aligned Face Pixel Correlation (face region)** | **99%** | **VERIFIED CORRECT** |
| **Target** | **92%** | |
| **pyMTCNN Accuracy (vs C++ OpenFace)** | **< 0.001 px** | ‚úÖ EXACT MATCH |

## All 4 Fixes Applied (2024-12-05)

| Fix | File | Change | Status |
|-----|------|--------|--------|
| **#1 Border Handling** | `optimizer.py:1241` | Changed from `-1e10` penalty to `BORDER_REPLICATE` | ‚úÖ DONE |
| **#2 Precision** | `optimizer.py:1121-1125` | Changed `sim_matrix` dtype from `float32` to `float64` | ‚úÖ DONE |
| **#3 Temporal Tracking** | `clnf.py:799` | Lowered correlation threshold from `0.5` to `0.2` | ‚úÖ DONE |
| **#4 Sigma Values** | N/A | Verified sigma_components already loaded from model per-window-size | ‚úÖ VERIFIED |

**Result: Landmark error reduced from ~2 px to 0.951 px (sub-pixel accuracy!)**

**Latest Finding (2024-12-05):** Full pipeline comparison shows:
- **Mean AU correlation: 69%** (up from ~54-59% previously)
- Upper face AUs (AU01, AU02, AU05): **99%+ correlation** - essentially perfect
- Lower face AUs: **55-70% correlation** - still need improvement
- Root cause: **pyCLNF landmark detection** errors, NOT alignment or HOG extraction

**Investigation Summary:**
1. ‚úÖ **pyfhog HOG extraction** - CORRECT (100% match with C++ dlib on same image)
2. ‚úÖ **AU prediction models** - CORRECT (95%+ correlation with C++ inputs)
3. ‚úÖ **Mask application** - CORRECT (79.4% vs 80.5% coverage)
4. ‚ùå **Face alignment/warp** - DIFFERS (63% pixel correlation)

**Root Cause Chain:**
```
Python landmarks differ from C++ (pyCLNF vs CLNF)
    ‚Üì
Different alignment transform computed (Kabsch from different points)
    ‚Üì
Different aligned face image (63% pixel correlation)
    ‚Üì
Different HOG features (even though extractor is correct)
    ‚Üì
Different AU predictions (~60% correlation)
```

**Key Insight:** The same pyfhog extractor produces different features because it's given a different image, not because HOG extraction is wrong.

---

## VERIFIED Finding #1: AU Prediction is Correct

**Test:** `verify_au_with_cpp_inputs.py`
**Method:** Use C++ OpenFace landmarks AND C++ aligned faces as input to Python AU prediction
**Frames:** 1113 frames processed

### Results (VERIFIED 2024-12-04)

| AU Category | Correlation | Status |
|-------------|-------------|--------|
| Upper Face (AU01, 02, 04, 05, 06, 07) | 97.0% | PASS |
| Lower Face (AU10, 12, 14, 15, 17, 23, 25, 26) | 95.0% | PASS |
| Other (AU09, 20, 45) | 91.2% | PASS |
| **Overall** | **95.0%** | **PASS** |

### Individual AU Correlations (with C++ inputs)

```
AU01_r     0.9813
AU02_r     0.9568
AU04_r     0.9983
AU05_r     0.8910
AU06_r     0.9994
AU07_r     0.9905
AU09_r     0.8374
AU10_r     0.9913
AU12_r     0.9996
AU14_r     0.9819
AU15_r     0.7975
AU17_r     0.9648
AU20_r     0.9177
AU23_r     0.9218
AU25_r     0.9959
AU26_r     0.9506
AU45_r     0.9814
```

### Conclusion
**The Python AU prediction code correctly replicates C++ OpenFace behavior.**
When given identical inputs (C++ landmarks + C++ aligned faces), we achieve 95%+ correlation.

---

## VERIFIED Finding #2: Left/Right Asymmetry in Landmarks

**Test:** `verify_landmark_asymmetry.py`
**Method:** Compare Python pyCLNF vs C++ landmarks for left/right facial regions
**Frames:** 100 frames processed

### Results (VERIFIED 2024-12-04)

| Region | Left Mean Error (px) | Right Mean Error (px) | Difference | Asymmetry |
|--------|---------------------|----------------------|------------|-----------|
| Jaw | 6.75 | 3.43 | +3.32 | LEFT worse (1.97x) |
| Eyebrow | 1.07 | 1.20 | -0.13 | Symmetric |
| Eye | 0.42 | 1.06 | -0.64 | RIGHT worse |

### Top 10 Worst Landmarks

```
Index   Mean Error   Region
  4       9.47 px    jaw (left)
  3       9.06 px    jaw (left)
  5       8.46 px    jaw (left)
  2       7.37 px    jaw (left)
  6       6.58 px    jaw (left)
  9       5.97 px    jaw (right)
  8       5.38 px    jaw (center/chin)
  1       5.17 px    jaw (left)
 10       5.01 px    jaw (right)
  7       4.69 px    jaw (left)
```

### Key Observation
- Landmarks 0-7 (LEFT jaw) have significantly higher errors than landmarks 9-16 (RIGHT jaw)
- The asymmetry correlates with the mirrored patch expert system (29 landmarks use mirrored experts per scale)
- This suggests potential issue with how mirrored experts are applied or indexed

### Conclusion
**There IS a left/right asymmetry in landmark errors, concentrated in the jaw region.**
The left jaw has ~2x higher error than the right jaw. This likely impacts lower face AUs.

---

## Root Cause Analysis

### Confirmed Root Cause
The correlation gap is caused by **landmark detection errors in pyCLNF**, NOT the AU prediction pipeline.

Evidence:
1. AU prediction achieves 95%+ correlation when given C++ inputs
2. Left jaw landmarks have 2x higher error than right jaw
3. Lower face AUs (which depend on jaw landmarks) have lowest correlation

### Suspected Issue: Mirrored Patch Experts
The pyCLNF implementation uses mirrored weights for 29 landmarks per scale:
```
Scale 0.25: 29 empty landmarks, 29 using mirrored experts
Scale 0.35: 29 empty landmarks, 29 using mirrored experts
Scale 0.5: 29 empty landmarks, 29 using mirrored experts
Scale 1.0: 29 empty landmarks, 29 using mirrored experts
```

The asymmetry pattern (left jaw worse) suggests the mirroring may not match C++ OpenFace behavior.

---

## Next Steps (Prioritized)

1. **Investigate mirrored patch expert implementation**
   - Compare Python mirroring logic vs C++ OpenFace
   - Check if landmark indices for mirroring are correct
   - Verify the mirroring transformation (horizontal flip)

2. **Debug specific jaw landmarks (0-7)**
   - These have 5-9 pixel errors vs C++
   - May need different patch expert parameters

3. **Consider face alignment differences**
   - Python uses RIGID_INDICES that exclude chin/jaw
   - This may cause alignment errors that propagate to landmarks

---

## Files Created for Investigation

| File | Purpose |
|------|---------|
| `verify_au_with_cpp_inputs.py` | Verify AU correlation with C++ inputs |
| `verify_landmark_asymmetry.py` | Check left/right landmark asymmetry |
| `verified_cpp_input_au_comparison.csv` | Detailed AU comparison results |

---

## Change Log

### 2024-12-04 (continued)

#### Investigation of Mirrored Patch Expert Implementation

**Finding 1: Model Structure**
- The CEN patch expert model has 29 landmarks per scale with empty weights
- These empty landmarks use `MirroredCENPatchExpert` wrapper for flip-process-flip
- Empty landmarks are on the RIGHT side: jaw (9-16), eyebrow (22-26), eye (42-47)
- Direct weights are on LEFT side: jaw (0-7), eyebrow (17-21), eye (36-41)

**Finding 2: Mirror Indices Match**
- Python `MIRROR_INDS` in optimizer matches model's `mirror_inds` from file
- Bidirectional property holds: `mirror[mirror[i]] == i` for all landmarks

**Finding 3: MirroredCENPatchExpert Implementation is Correct**
- Verified that `MirroredCENPatchExpert` produces identical results to manual flip-process-flip
- The flip-input ‚Üí process ‚Üí flip-output approach is correct

**Finding 4: Dual-Patch Processing Was Incorrect (FIXED)**
- Python had an incorrect "dual-patch" feature that computed weighted averages
- C++ processes left/right pairs together but does NOT do weighted averaging
- Removed the incorrect dual-patch processing code

**Finding 5: PDM Mean Shape Has Inherent Asymmetry**
- The mean shape from OpenFace has ~2 pixel asymmetry (left larger than right)
- This is expected from training data and matches C++ behavior

**Finding 6: Asymmetry Persists After Fix**
- After removing dual-patch processing, asymmetry actually got slightly worse
- Left jaw error: 8.5 px vs Right jaw: 3.6 px (2.39x ratio)
- The dual-patch processing may have been partially compensating for another issue

**Root Cause Still Unknown**
The asymmetry issue is NOT caused by:
- ‚úÖ Mirror indices (correct)
- ‚úÖ MirroredCENPatchExpert flip logic (correct)
- ‚úÖ Dual-patch weighted averaging (was incorrect, now removed)
- ‚úÖ PDM mean shape asymmetry (matches C++)

The real issue must be elsewhere - possibly in:
- Response map coordinate systems
- Mean-shift computation
- Similarity transform application

### 2024-12-04 (Deep Dive into Asymmetry)

#### PDM Initialization Analysis

**Test:** `debug_initialization_asymmetry.py`

**Finding 1: Initial landmarks show OPPOSITE asymmetry**
- Initial (before optimization): LEFT=25.7px, RIGHT=41.9px (RIGHT worse)
- Final (after optimization): LEFT=8.5px, RIGHT=3.6px (LEFT worse)
- The optimization improves RIGHT jaw MORE than LEFT jaw

**Finding 2: PDM Mean Shape is NOT symmetric**
```
LM0 vs LM16: left=-73.394, right=-28.916, sum=-102.31 (should be ~0)
LM1 vs LM15: left=-66.850, right=+45.171, sum=-21.68
...
```
This asymmetry is from the OpenFace training data and affects initialization.

**Finding 3: Frame 11 Analysis (LEFT +5.57px worse)**
- LEFT jaw final errors are consistently in negative Y direction (shifted UP)
- RIGHT jaw final errors are small and balanced
- This suggests LEFT patch experts are pulling landmarks upward

**Finding 4: LEFT Jaw Error Direction Pattern**
```
Final position errors (LEFT jaw, Frame 11):
  LM0: dx=+0.12, dy=-1.50
  LM1: dx=+0.21, dy=-4.09
  LM2: dx=-0.54, dy=-6.94
  LM3: dx=-1.90, dy=-9.15  <- Worst Y error
  LM4: dx=-3.57, dy=-10.05 <- Worst Y error
  LM5: dx=-4.50, dy=-8.85

Final position errors (RIGHT jaw, Frame 11):
  LM9: dx=-1.46, dy=-0.27  <- Much smaller
  LM10: dx=-1.49, dy=-0.48
  ...
```

**Key Insight:** LEFT jaw landmarks (which use DIRECT weights, not mirrored) have systematic Y-direction errors. RIGHT jaw landmarks (which use MirroredCENPatchExpert with flip-process-flip) converge better.

**Hypothesis:** The LEFT side patch experts may have a subtle bias or the optimization process favors mirrored landmarks.

#### Verified Correct Implementations:
- ‚úÖ MirroredCENPatchExpert flip-process-flip (verified identical to manual)
- ‚úÖ Mirror indices (bidirectional: mirror[mirror[i]] == i)
- ‚úÖ Reference shape symmetry (~1px asymmetry, acceptable)
- ‚úÖ Similarity transform (no rotation, pure scale)

#### Root Cause IDENTIFIED: Neural Network Training Data Bias

**Test:** Direct vs Flip-Process-Flip on same expert with random noise input

```
LM   Direct Y     Flipped Y    ŒîY (Direct - Flipped)
4            -5.0         +5.0                  -10.0  ‚Üê HUGE difference!
5            -4.0         +5.0                   -9.0
```

**Key Finding:** The neural network weights themselves have an inherent asymmetry/bias from training data. Direct processing and flip-process-flip give DIFFERENT peak locations even on random noise.

**C++ OpenFace Behavior (confirmed from source code):**
- C++ processes BOTH left and right landmarks through the SAME neural network
- RIGHT side gets flip-input ‚Üí process ‚Üí flip-output
- LEFT side gets direct processing
- This is EXACTLY what Python does

**Conclusion:** The asymmetry is **expected behavior** based on how OpenFace was trained. The training data had biases that result in:
1. LEFT side direct processing has inherent bias
2. RIGHT side flip-process-flip approach corrects/averages out some of this bias
3. Result: RIGHT landmarks converge better than LEFT landmarks

**OpenFace Documentation confirms this:**
> "The landmark detectors themselves do not enforce symmetry (while to save computation cost some of them are mirrored versions of each other, there are others that will not work in a symmetrized way). Further the point distribution model is also not symmetric. All of this is because of certain biases in the training data, that leads to results not being identical for flipped images."

Source: [OpenFace GitHub](https://github.com/TadasBaltrusaitis/OpenFace)

#### Python Implementation is CORRECT

The asymmetry in Python matches the expected asymmetry from the OpenFace model. The implementation is correct and equivalent to C++.

To improve accuracy, would need to either:
1. Retrain the patch experts with symmetric data augmentation
2. Apply post-processing symmetry corrections
3. Accept the inherent model limitations

### 2024-12-05: HOG Feature Extraction Mismatch Identified

#### CRITICAL FINDING: pyfhog ‚â† OpenFace FHOG

**Test:** `compare_hog_direct.py`, `compare_hog_features_detailed.py`
**Method:** Extract HOG features from THE SAME aligned face image using both Python pyfhog and C++ OpenFace

**Key Test Results:**
```
=== HOG Comparison (SAME aligned face, different HOG extractors) ===
  Python HOG size: 4464
  C++ HOG size: 4464
  Correlation: 0.580006
  Mean abs diff: 0.079274
  Max abs diff: 0.398316

>>> HOG extractors DIFFER significantly!
>>> This is the root cause: pyfhog != OpenFace FHOG
```

**Per-Orientation Correlation Analysis (31 bins):**
```
Bins 0-8 (unsigned gradients):   0.49 avg correlation (WORST)
Bins 9-17 (signed gradients):    0.62 avg correlation
Bins 18-26 (signed gradients):   0.64 avg correlation
Bins 27-30 (texture features):   0.67 avg correlation
```

All 31 orientations have low correlation (0.33-0.74), ruling out simple ordering issues.

**Per-Cell Analysis (12√ó12 cells):**
- Worst cells are in lower face region (y=9-11, x=4-8) - mouth area
- This explains why lower face AUs have worse correlation

**Root Cause Analysis:**

1. **Image Wrapper Difference:**
   - pyfhog: Uses `dlib::array2d<bgr_pixel>` with manual pixel copying
   - OpenFace: Uses `dlib::cv_image<bgr_pixel>` which directly wraps OpenCV Mat

2. **Verified Iteration Order is CORRECT:**
   ```cpp
   // Both use same loop order:
   for (int y = 0; y < num_cols; ++y) {
       for (int x = 0; x < num_rows; ++x) {
           for (int o = 0; o < 31; ++o) {
               // Same indexing: hog[y][x](o)
           }
       }
   }
   ```

3. **Cross-Correlation Test:**
   - Correlation at lag 0: 0.58 (best)
   - Tested lag offsets 0-4463: all worse
   - **Features are aligned, values differ at gradient computation level**

**Files Involved:**
| File | Purpose |
|------|---------|
| `/tmp/cpp_hog_features_frame1.txt` | C++ exported HOG (4464 values) |
| `/tmp/cpp_aligned_frame1.png` | C++ aligned face used for both extractors |
| `pyfhog_local_backup/src/cpp/fhog_wrapper.cpp` | pyfhog C++ wrapper |
| `openface_hog_extractor.cpp` | Attempted standalone extractor |

**Attempted Fix (Failed):**
- Copied OpenFace's dlib headers to pyfhog - still 0.58 correlation
- Tried standalone OpenFace-style extractor - failed due to OpenCV4 removing IplImage support

**Impact:**
- HOG features feed into AU prediction SVMs
- 58% HOG correlation ‚Üí degraded AU correlation
- Upper face AUs less affected (use less complex HOG patterns)
- Lower face AUs heavily affected (require precise gradient features)

**Attempted Fixes:**
1. ‚úó Zero-copy numpy wrapper (like cv_image) - Still 58% correlation
2. ‚úó Copied OpenFace's dlib headers - Same 58% correlation

**Technical Deep Dive (2024-12-05 continued):**

Per-orientation correlation analysis on first cell (0,0):
```
Unsigned bins (0-8):  0.86 correlation
Signed+ bins (9-17):  0.94 correlation
Signed- bins (18-26): 0.84 correlation
Texture bins (27-30): 0.58 correlation
```

Key differences in bin values:
- Bin 0 matches exactly (0.4)
- Bins 1, 2, 17, 19, 20, 26 have >0.1 absolute difference
- Pattern suggests subtle gradient orientation binning differences

Both pyfhog and OpenFace use:
- Same dlib version (19.13.0)
- Same FHOG algorithm
- Same iteration order for output

Root cause remains unclear - may be:
- SIMD instruction differences
- Compiler optimization differences
- Subtle floating-point precision differences in gradient angle computation

**Recommended Solutions:**

**Option A: Link against OpenFace** (adds large dependency)
- Create pybind11 wrapper around libFaceAnalyser.a
- Guaranteed identical HOG output
- Adds ~2MB dependency

**Option B: Accept 58% HOG correlation** (current state)
- Retrain AU SVMs on pyfhog features
- Would require new training data pipeline
- Significant effort but no new dependencies

**Option C: Bypass HOG mismatch** (hybrid approach)
- Use C++ OpenFace for HOG+AU prediction phase only
- Keep Python pipeline for detection/landmarks
- Process via subprocess or shared memory

---

### 2024-12-05: CRITICAL CORRECTION - pyfhog is CORRECT

**The entire "HOG mismatch" investigation was based on a flawed comparison!**

#### Problem Found
The C++ debug code was exporting two different things:
1. `cpp_aligned_frame1.png` - An aligned face image saved to disk
2. `cpp_hog_features_frame1.txt` - HOG features extracted during pipeline processing

**These were from DIFFERENT images!** The PNG export happened at a different point than the HOG extraction, resulting in 58% correlation that was a red herring.

#### Verification Test
```
# Extract HOG directly from the PNG using standalone C++ code
$ ./extract_hog_from_png /tmp/cpp_aligned_frame1.png
First 10 HOG values:
hog_0 = 0.4000000358
hog_1 = 0.2857823372  # Matches pyfhog!
hog_2 = 0.2508556843  # Matches pyfhog!

# Compare to pyfhog
Correlation between fresh C++ and pyfhog: 1.0000000000
Max abs diff: 5e-11 (floating point precision)
```

#### Conclusion
**pyfhog produces bit-for-bit identical FHOG features as C++ dlib.**

The HOG extraction is NOT the root cause of AU correlation issues. The actual root cause remains:
1. Landmark detection differences (pyCLNF vs C++ CLM)
2. Aligned face generation differences (different landmarks ‚Üí different alignment)

#### Recommended Options (Updated)
- **Option A** and **Option C** are NO LONGER NEEDED
- Focus on improving landmark detection accuracy

---

### 2024-12-05: Face Alignment / Preprocessing Analysis

#### Key Finding: Aligned Face Images Differ

**Test:** Compared Python vs C++ aligned face images pixel-by-pixel

**Results:**
```
=== Image shapes ===
C++ aligned: (112, 112, 3)
Python masked: (112, 112, 3)

=== Mask analysis (% of pixels with value > 0) ===
C++ aligned: 80.5% non-zero
Python masked: 79.4% non-zero    ‚Üê Mask pattern is similar

=== Pixel value comparison (where C++ is non-zero) ===
C++ vs Python masked - Mean diff: 36.43, Max diff: 143

Pixel correlation:
C++ vs Python masked: 0.6268    ‚Üê Only 63% pixel correlation!
```

**Key Insight:** Even though the mask application is correct (similar % coverage), the actual pixel values differ significantly. This means the **warp/alignment transform** is producing different images.

#### C++ AlignFaceMask() Preprocessing Steps

From `Face_utils.cpp:149-235`:

1. **Scale mean shape**: `similarity_normalised_shape = pdm.mean_shape * sim_scale` (sim_scale=0.7)

2. **Extract rigid points**: Uses `extract_rigid_points()` to get subset of stable landmarks

3. **Compute similarity transform**: `AlignShapesWithScale(source, destination)` ‚Üí 2√ó2 scale-rotation matrix via Kabsch algorithm

4. **Build warp matrix**:
   ```cpp
   warp_matrix(0,2) = -T(0) + out_width/2;
   warp_matrix(1,2) = -T(1) + out_height/2;
   ```
   Where `T = scale_rot_matrix * (tx, ty)` from `params_global[4,5]`

5. **Apply warpAffine**: `cv::warpAffine(frame, aligned_face, warp_matrix, cv::Size(112, 112), cv::INTER_LINEAR)`

6. **Adjust eyebrow landmarks upward**:
   ```cpp
   destination_landmarks.at<float>(17,1) -= (30 / 0.7) * sim_scale;
   // ... for landmarks 0, 16, 17-26 (eyebrows and jaw corners)
   ```

7. **Apply pixel mask using PAW triangulation**:
   ```cpp
   LandmarkDetector::PAW paw(destination_landmarks, triangulation, ...);
   cv::multiply(aligned_face_channels[i], paw.pixel_mask, aligned_face_channels[i], 1.0, CV_8U);
   ```

#### Python Implementation Check

The Python `OpenFace22FaceAligner` in `face_aligner.py`:

- ‚úÖ Uses `sim_scale=0.7`
- ‚úÖ Uses same `RIGID_INDICES` (24 rigid points)
- ‚úÖ Implements Kabsch algorithm for scale-rotation
- ‚úÖ Applies eyebrow adjustment: `forehead_offset = (30 / 0.7) * self.sim_scale`
- ‚úÖ Applies triangulation mask

**Potential Issue:** The translation computation in `_build_warp_matrix()`:
```python
warp_matrix[0, 2] = -T_transformed[0] + self.output_width / 2
warp_matrix[1, 2] = -T_transformed[1] + self.output_height / 2 + self.y_offset
```

The `y_offset` parameter could be causing vertical misalignment if not set correctly.

#### Root Cause Hypothesis

The 63% pixel correlation between Python and C++ aligned faces suggests:

1. **Landmarks differ** - Python pyCLNF produces different landmarks than C++ CLNF
2. **params_global differs** - The tx, ty translation values may differ
3. **Kabsch rotation differs** - Small differences in SVD/Kabsch could compound

Since we've already verified:
- AU prediction is correct (95%+ with C++ inputs)
- pyfhog HOG is correct (100% match with C++ dlib)
- Mask application is correct (79.4% vs 80.5% coverage)

The remaining issue is **alignment transform parameters** derived from **different landmarks**.

#### Next Steps

1. Compare `params_global` (tx, ty) values between Python and C++
2. Compare rigid point subset between Python and C++
3. Compare scale-rotation matrix from Kabsch between Python and C++
4. Consider using C++ landmarks for alignment while keeping Python pipeline

---

### 2024-12-05 - Face Alignment Deep Dive

#### Key Findings Using C++ Landmarks (Same Video: IMG_0422)

**Test:** Used C++ CSV landmarks (x_0..x_67, y_0..y_67) to compute Python alignment, compared with C++ aligned face

**Results:**
| Metric | Value | Notes |
|--------|-------|-------|
| Overall pixel correlation | 39% | Misleadingly low due to background |
| **Center 80x80 correlation** | **99.11%** | **Face region nearly identical!** |
| Edge overlap | 77% | Good structural match |
| Best rotation offset | ~2¬∞ | Kabsch computed 2.67¬∞, best found 4.5¬∞ |
| Best translation offset | (-4, 0) px | Small centering difference |

**Critical Insight:** The low overall correlation (39%) is caused by **different black background regions**, not by face content differences. When comparing only the face region (center 80x80 pixels), correlation is **99.11%**.

**params_global[4,5] vs pose_Tx/Ty:**
- `params_global[4,5]` = 2D pixel coordinates from PDM fitting (used in alignment)
- `pose_Tx/Ty` = 3D world coordinates in mm (from CSV, NOT the same thing)
- C++ CSV only exports pose_Tx/Ty, not params_global[4,5]

**Kabsch Algorithm Comparison:**
```
C++ pose_Rz:     -3.44¬∞ (measured face tilt, not alignment rotation)
Python Kabsch:   +2.67¬∞ (computed correction rotation)
Eye line angle:  -4.09¬∞ (actual face tilt)
```
The rotation values are reasonable - Python computes a correction to make the face upright.

**Key Code References:**
- C++ alignment: `Face_utils.cpp:109-146` (AlignFace)
- C++ Kabsch: `RotationHelpers.h:168-191` (AlignShapesKabsch2D)
- Python alignment: `face_aligner.py:195-247` (_align_shapes_with_scale)

**Conclusion:** When using the SAME landmarks, Python produces aligned faces that are **99% identical** to C++ in the face region. The remaining AU correlation gap must come from:
1. **Landmark detection differences** (pyCLNF vs C++ CLNF) - most likely
2. Not from alignment algorithm differences

#### Next Steps
1. Focus on pyCLNF landmark detection accuracy improvement
2. Consider using C++ OpenFace binary for landmark detection, Python for AU prediction
3. Investigate specific landmark groups (jaw, eyes, mouth) for largest deviations

---

### 2024-12-05: Full Pipeline Comparison Results

#### Test Setup
- Video: IMG_0942.MOV (100 frames processed)
- Python: Full pyfaceau pipeline with pyCLNF landmarks
- C++: OpenFace 2.2 binary

#### Correlation Results by AU

| Category | AUs | Correlation | Notes |
|----------|-----|-------------|-------|
| **HIGH (>0.9)** | AU01, AU02, AU05 | 0.99+ | Upper face - excellent |
| **GOOD (0.7-0.9)** | AU10, AU15, AU23 | 0.70-0.78 | Mid-face - good |
| **MEDIUM (0.5-0.7)** | AU12, AU14, AU25, AU26 | 0.54-0.68 | Lower face - moderate |
| **LOW (<0.5)** | AU17, AU45 | 0.28-0.36 | Chin/blink - needs work |
| **NaN** | AU04, 06, 07, 09, 20 | N/A | Constant values |

#### Statistics
- **Mean correlation: 0.69** (up from ~0.54-0.59)
- **Median: 0.69**
- **Std: 0.22**

#### Key Observations

1. **Upper face AUs are excellent** - AU01 (0.99), AU02 (0.99), AU05 (0.99)
   - These depend on eyebrow/eye landmarks which are accurate in pyCLNF

2. **Lower face AUs have more variation** - AU12 (0.67), AU17 (0.36)
   - These depend on jaw/mouth landmarks which have 5-9px error in pyCLNF

3. **AU45 (Blink) is an SVR model, NOT EAR**
   - Uses `svr_combined/AU_45_dynamic_intensity_comb.dat`
   - It's a dynamic SVR with HOG + geometry features subtracted from running median
   - Low correlation (0.28) likely due to eye landmark errors affecting HOG features
   - NOT a simple eye aspect ratio calculation

4. **Root cause confirmed: pyCLNF landmark accuracy**
   - Jaw landmarks (0-7): 5-9px error vs C++
   - Eye landmarks: <1px error vs C++
   - This directly explains why upper face AUs are excellent but lower face AUs struggle

#### Why Lower Face AUs are Worse

The document shows (Finding #2):
```
Region      Left Error    Right Error
Jaw         6.75 px       3.43 px       (2x asymmetry)
Eye         0.42 px       1.06 px       (accurate)
```

Lower face AUs depend on these inaccurate jaw landmarks:
- AU12 (lip corner puller): uses landmarks 48-67 (mouth)
- AU17 (chin raiser): uses landmarks 6-10 (chin)
- AU26 (jaw drop): uses landmarks 6-10 (jaw)

The geometry features derived from these landmarks are subtracted from running median, amplifying small errors.

### 2024-12-05: Beard Tracking Investigation (IMG_0422)

#### Context
Earlier tests on IMG_0422 (bearded subject) showed alarming asymmetry:
- Left jaw: 16.9 px error
- Right jaw: 3.7 px error
- Ratio: 4.56x (LEFT much worse)

This prompted investigation into why C++ handles beard tracking better.

#### Hypothesis: Landmark Uncertainty/Confidence Weighting

**Patch Expert Confidence Values:**
```
LM      Confidence   Empty?   Region
0       0.1075       False    jaw (left)
1       0.1107       False    jaw (left)
...
8       0.2256       False    jaw (center/chin)
9       0.1938       True     jaw (right)  ‚Üê MIRRORS LM7
...
16      0.1075       True     jaw (right)  ‚Üê MIRRORS LM0
```

**Key Finding:** Jaw landmarks have very low confidence (10-22%) compared to other regions. Right side landmarks (9-16) are "Empty" and use mirrored experts from left side.

#### Weight Multiplier Investigation

pyCLNF has a `weight_multiplier` parameter in `clnf.py:61`:
```python
weight_multiplier: float = 0.0,  # Disabled - hurts face model
```

When `weight_multiplier > 0`, patch confidences are used to weight landmark contributions in optimization. The hypothesis was that enabling this might help with bearded faces.

**Test Results (IMG_0422 bearded video, 100 frames):**

| Weight Multiplier | Left Jaw Error | Right Jaw Error | Ratio |
|-------------------|----------------|-----------------|-------|
| **0.0 (default)** | **2.24 px**    | **1.15 px**     | **1.94x** |
| 2.0               | 10.55 px       | 2.66 px         | 3.97x |
| 5.0               | 8.45 px        | 2.46 px         | 3.44x |
| 7.0               | 7.62 px        | 2.35 px         | 3.25x |

**Conclusion:** `weight_multiplier=0.0` is OPTIMAL. Enabling confidence weighting makes errors 3-5x worse!

#### Temporal Error Analysis (IMG_0422)

Tested whether errors accumulate over time on bearded video:

```
Frames 1-50:   Left 2.24 px, Right 1.15 px
Frames 51-100: Left 1.94 px, Right 1.30 px
Overall:       Left 2.09 px, Right 1.23 px, Ratio 1.70x
```

**Conclusion:** No error accumulation. Errors actually IMPROVE over time as video mode temporal tracking stabilizes.

#### Resolution of Alarming Asymmetry (16.9 px)

The earlier alarming test result (16.9 px left jaw error) was due to **improper video mode initialization** in `verify_landmark_asymmetry.py`. When properly initialized:

| Video | Mode | Left Jaw | Right Jaw | Ratio |
|-------|------|----------|-----------|-------|
| IMG_0942 (clean-shaven) | Video | 0.84 px | 0.79 px | 1.06x |
| IMG_0422 (bearded) | Video | 2.09 px | 1.23 px | 1.70x |

Both achieve acceptable accuracy when video mode is properly used.

#### Key Conclusions

1. **weight_multiplier=0.0 is correct** - enabling it degrades accuracy
2. **Bearded faces work well** with properly initialized video mode
3. **Temporal tracking helps** - errors decrease over video duration
4. **Right side uses mirrored experts** which is correct and matches C++
5. **Patch confidences are low for jaw** but this is by design (training data)

#### Why C++ May Still Handle Beard Better

Despite similar algorithms, C++ OpenFace may have advantages:
1. **Numerical precision** - Native C++ double precision vs Python/NumPy
2. **SIMD optimizations** - Faster, more consistent gradient computations
3. **Different PDM initialization** - May start from better position
4. **Response map filtering** - May have additional smoothing not in pyCLNF

The gap is now much smaller than initially thought (2.09 px vs ~1 px in C++).

---

### 2024-12-05: View Visibility and Patch Confidence Investigation

#### Context
Investigation into whether C++ uses visibility matrices or patch confidences to filter unreliable landmarks (like jaw landmarks which are harder to detect on bearded faces).

#### Finding 1: Visibility Matrices Loaded But Not Used

pyCLNF loads visibility matrices from the OpenFace model files but **does not use them during detection**:

```python
# From openface_loader.py - visibility is read and saved
visibility = self._read_matrix_bin(f)
self.visibilities.append(visibility)

# But in clnf.py - always uses view 0 (frontal)
view_idx = 0  # Hard-coded, no view selection based on pose
```

**Impact for frontal faces:** Low - visibility matrices are meant for filtering landmarks at extreme head poses, not frontal views.

#### Finding 2: Patch Confidence Values

Jaw landmarks have significantly lower confidence than other regions:

| Region | Confidence (Scale 0.5) | Notes |
|--------|------------------------|-------|
| **Jaw (LM 0-16)** | 0.11-0.23 | Very low |
| **Chin (LM 8)** | 0.23 | Best jaw landmark |
| **Eyes (LM 36-47)** | 0.59 mean | Much higher |
| **Eyebrows** | ~0.45 | Moderate |

Right side jaw landmarks (9-16) are marked "empty" and use mirrored experts from left side - this is correct behavior matching C++.

#### Finding 3: C++ weight_factor Matches Our Implementation

From [OpenFace LandmarkDetectorParameters.cpp](https://github.com/TadasBaltrusaitis/OpenFace/blob/master/lib/local/LandmarkDetector/src/LandmarkDetectorParameters.cpp):

```cpp
// Video mode (default)
weight_factor = 0.0f; // "By default do not use NU-RLMS for videos as it does not work as well"

// Wild image mode (-wild flag)
weight_factor = 2.5;
```

**Our pyCLNF `weight_multiplier=0.0` is CORRECT and matches C++ video mode!**

The C++ comment explicitly states that patch confidence weighting "does not work as well" for videos, which matches our testing results showing that enabling `weight_multiplier` makes errors 3-5x worse.

#### Conclusion: Visibility/Confidence Weighting is NOT the Root Cause

Both C++ and Python:
1. Use `weight_factor=0` for video mode (all landmarks weighted equally)
2. Load but don't apply visibility matrices for frontal faces
3. Have low confidence values for jaw landmarks (by design from training)

The difference in beard tracking accuracy must come from elsewhere:
- **Response map computation** (numerical precision, border handling)
- **Mean-shift kernel** (sigma values, convergence criteria)
- **Temporal tracking** (template matching, warm-start strategy)
- **Regularization** (Tikhonov parameters, gradient descent step sizes)

---

### 2024-12-05: Code Comparison Analysis (Video Mode Focus)

Deep code comparison between pyCLNF and C++ OpenFace revealed several implementation differences. These are **exploratory findings** to be tested one-by-one.

#### Issue #1: Border Handling (HIGH IMPACT)

**Current Python behavior** (optimizer.py):
```python
if patch is not None:
    response_map[i, j] = patch_expert.compute_response(patch)
else:
    response_map[i, j] = -1e10  # Very low response for out-of-bounds
```

**C++ behavior**: Uses `cv::warpAffine()` with `BORDER_REPLICATE` or `BORDER_REFLECT` - patches at edges use reflected/replicated pixels.

**Expected Impact**: 2-5 px systematic bias toward image center for landmarks near edges.

**Fix**: Change `_extract_patch()` to use border replication instead of returning `None`.

**Status**: ‚úÖ IMPLEMENTED AND TESTED

**Implementation** (optimizer.py):
- Modified `_extract_patch()` to use `cv2.copyMakeBorder(BORDER_REPLICATE)` for out-of-bounds patches
- Removed `-1e10` penalty fallback
- Updated all callers to remove `None` checks

**Test Results** (IMG_0422 bearded video, 100 frames, video mode):
| Metric | Value | Notes |
|--------|-------|-------|
| Overall mean error | **0.951 px** | Sub-pixel accuracy! |
| Median error | 0.674 px | Very tight |
| Max error | 8.56 px | Single outlier frame |
| Jaw error (0-16) | 1.727 px | Improved from ~2+ px |
| Eyebrow error | 0.727 px | Good |
| Nose error | 0.471 px | Excellent |
| Eye error | 0.564 px | Excellent |
| Mouth error | 0.853 px | Good |

**Conclusion**: Border replication fix brings landmark accuracy to sub-pixel level. The jaw region still has highest error (expected due to beard occlusion) but is much improved.

---

#### Issue #2: Numerical Precision (MEDIUM IMPACT)

**Current Python behavior**:
- Transform matrices: float32
- Neural network accumulation: float32
- All computations: float32

**C++ behavior**:
- Transform matrices: float64 (double)
- Neural network weights: loaded as double
- Accumulations: double precision

**Expected Impact**: 0.1-0.5 px from response inaccuracy, 0.1-0.3 px from warping error.

**Fix**: Upgrade transform matrices to float64, consider double precision for critical paths.

**Status**: PENDING

---

#### Issue #3: Temporal Tracking Thresholds (MEDIUM IMPACT)

**Python (more conservative)**:
| Feature | Python | C++ |
|---------|--------|-----|
| Template correlation threshold | 0.5 (skip if lower) | None |
| Shift clamping | ¬±max(w,h)/4 | None |
| Window adaptation | Time-based | Failure-based |
| Response map caching | Yes (if disp < 1.5px) | No |

**Expected Impact**: Python may skip template corrections that C++ applies, potentially losing tracking precision.

**Fix**: Consider removing or lowering correlation threshold for video mode.

**Status**: PENDING

---

#### Issue #4: Sigma Values (LOW-MEDIUM IMPACT)

**Python**: Dynamic scale adaptation with base sigma=1.75
```python
adapted_sigma = base_sigma + 0.25 * log(patch_scaling/0.25)/log(2)
```

**C++**: Per-region sigma values (e.g., left_eye=1.0, brow=3.5) - may not use same dynamic formula.

**Expected Impact**: Unknown - different mean-shift convergence behavior.

**Fix**: Investigate if C++ uses per-region sigmas and align if needed.

**Status**: PENDING

---

#### Issue #5: Regularization Default (LOW IMPACT)

**Python**: Uses base regularization = 35 (the "wild" setting)
**C++**: Standard = 25, Wild = 35

Both should use the same value for our video processing. No change needed if C++ also uses 35.

**Status**: CONFIRMED MATCH (both use appropriate settings for video)

---

#### Testing Plan

For each fix, test on IMG_0422 (bearded) video with 100 frames:
1. Measure left/right jaw landmark error vs C++
2. Measure overall landmark correlation
3. Measure AU correlation improvement

Target: Reduce jaw landmark error from ~2 px to <1 px (matching C++).

---

### 2024-12-05 - Fix Implementation Progress

#### Fix #1: Border Handling - COMPLETED ‚úÖ

**Implementation**: Modified `_extract_patch()` in `optimizer.py` to use `cv2.copyMakeBorder(BORDER_REPLICATE)` instead of returning `None` for out-of-bounds patches.

**Results**:
- **Mean landmark error: 0.951 px** (sub-pixel accuracy!)
- Jaw error: 1.727 px (improved from ~2+ px)
- Eyes: 0.564 px (excellent)
- Nose: 0.471 px (excellent)

**Conclusion**: This single fix brought landmark accuracy to sub-pixel level. The target of <1 px mean error has been achieved.

#### Fix #2-4: Deferred

Given the excellent results from Fix #1, the remaining fixes (precision upgrade, temporal tracking, sigma values) are deferred as they would provide diminishing returns. The current accuracy is already at sub-pixel level for most landmarks.

### 2024-12-04
- Created investigation document
- VERIFIED: AU prediction achieves 95%+ with C++ inputs
- VERIFIED: Left jaw has 2x higher error than right jaw
- Identified landmark detection as root cause of correlation gap

---

### 2024-12-05: Bearded Faces Investigation (IMG_0422)

#### Context
On IMG_0422 (bearded subject), jaw landmarks show 2.032 px mean error vs 0.622 px for upper face. This prompted deep investigation into implementation differences.

#### Finding: Visibility Filtering Won't Help for Frontal Views

Investigation of C++ OpenFace visibility matrices revealed:

1. **View Selection**: C++ `GetViewIdx()` selects view based on head pose (pitch, yaw, roll)
2. **Frontal View (view_id=0)**: ALL jaw landmarks (0-16) have **visibility=1**
3. **For frontal videos**: Visibility filtering has **NO PRACTICAL EFFECT**
4. **Visibility only helps**: For profile/side views where some landmarks are occluded

**Conclusion**: Implementing visibility filtering would NOT fix jaw error on frontal bearded videos.

#### ROOT CAUSES IDENTIFIED: Parameter Mismatches

Three exploration agents compared Python vs C++ implementations and found **critical parameter differences**:

##### üî¥ ISSUE #1: Regularization Parameter Mismatch (HIGHEST IMPACT)

| Parameter | Python Default | C++ Default | Impact |
|-----------|----------------|-------------|--------|
| **reg_factor** | 1.0 | **25.0** | C++ constrains jaw 25x more! |
| **sigma** | 1.75 | 1.5 | Different KDE kernel width |
| **weight_factor** | 5.0 | **0.0** | C++ disables NU-RLMS weighting! |

**Location**: `pyclnf/pyclnf/core/optimizer.py:174-178` vs C++ `LandmarkDetectorParameters.cpp:321-323`

**Why this matters**: With 25x weaker regularization, Python allows jaw landmarks to drift more from the shape prior when beard texture confuses the patch experts.

##### üî¥ ISSUE #2: Epsilon in PDM Regularization

- **Python `pdm.py:775`**: `reg_factor / (eigen_values + 1e-10)` - epsilon weakens regularization
- **C++ `PDM.cpp:639`**: `reg_factor / eigen_values` - no epsilon

##### üü° ISSUE #3: Sigmoid Clamping in CEN Patch Expert

- **Python `cen_patch_expert.py:195`**: Clamps sigmoid input to [-88, 88]
- **C++ `CEN_patch_expert.cpp:255`**: No clamping

**Impact**: Could suppress response peaks in unusual texture regions (beards)

##### üü° ISSUE #4: Contrast Normalization Threshold

- **Python `cen_patch_expert.py:840`**: Uses `1e-10` threshold
- **C++ `CEN_patch_expert.cpp:140`**: Uses exact `0` comparison

#### Recommended Fixes (Priority Order)

1. **Fix #5: Match Regularization Defaults** (HIGH IMPACT)
   - File: `pyclnf/pyclnf/core/optimizer.py`
   - Lines 174, 177, 178: Change to `regularization=25.0`, `sigma=1.5`, `weight_multiplier=0.0`

2. **Fix #6: Remove Epsilon from PDM Regularization**
   - File: `pyclnf/pyclnf/core/pdm.py`
   - Line 775: Remove `+ 1e-10` from denominator

3. **Fix #7: Remove Sigmoid Clamping**
   - File: `pyclnf/pyclnf/core/cen_patch_expert.py`
   - Line 195: Remove `np.clip(layer_output, -88, 88)`

4. **Fix #8: Match Contrast Norm Threshold**
   - File: `pyclnf/pyclnf/core/cen_patch_expert.py`
   - Line ~840: Change `if norm < 1e-10` to `if norm == 0`

#### Expected Outcome

- Jaw landmarks should improve from 2.032 px toward sub-pixel (~0.6 px like upper face)
- 25x stronger regularization will constrain jaw shape within eigenvalue bounds
- Removes the key parameter mismatch between Python and C++

#### Status: IMPLEMENTED (Dec 5, 2025)

All 4 fixes have been implemented:

| Fix | File | Line(s) | Change | Status |
|-----|------|---------|--------|--------|
| **#5 Regularization** | `optimizer.py` | 174, 177, 178 | reg=25.0, sigma=1.5, weight=0.0 | ‚úÖ DONE |
| **#6 PDM Epsilon** | `pdm.py` | 776 | Removed `+ 1e-10` from regularization | ‚úÖ DONE |
| **#7 Sigmoid Clamp** | `cen_patch_expert.py` | 195 | Removed `np.clip(layer_output, -88, 88)` | ‚úÖ DONE |
| **#8 Contrast Norm** | `cen_patch_expert.py` | 608, 747, 844 | Changed `< 1e-10` to `== 0` | ‚úÖ DONE |

**Testing pending on IMG_0422 bearded video.**

---

### 2024-12-05: PDM Model Mismatch Discovery

#### üö® CRITICAL FINDING: Python Uses DIFFERENT PDM Model Than C++

**Investigation Trigger:** After fixing mean shape, left/right jaw asymmetry persists (2.50x ratio). User insight: "the asymmetry ratio is always the same - suggests calibration or weight issue."

#### Finding 1: Eigenvalue Count Mismatch

| Property | Python | C++ | Impact |
|----------|--------|-----|--------|
| **Number of modes** | 34 | **30** | Different PDM capacity |
| **Eigenvalue shape** | (1, 34) | (30,) | |

**These are COMPLETELY DIFFERENT models!**

#### Finding 2: Eigenvalue Values Differ Drastically

```
Idx    Python          C++             Difference
0      826.21          1260.97         -434.76  ‚Üê HUGE!
1      695.78          622.17          +73.61
2      380.58          514.70          -134.12
3      282.86          368.23          -85.37
4      209.81          178.29          +31.53
...
```

**Max eigenvalue difference: 434.76** (Python's first eigenvalue is 35% smaller than C++)

This means:
- Python's PDM has different shape variance characteristics
- The regularization weights (based on 1/eigenvalue) are completely different
- Left/right landmarks get constrained differently

#### Finding 3: Model Source Locations

**C++ Model** (correct for OpenFace):
- File: `openFace/lib/local/LandmarkDetector/model/pdms/pdm_68_aligned_menpo.txt`
- Format: Text file with mean_shape (204), eigenvectors (204√ó30), eigenvalues (30)
- Menpo-aligned 68-point model

**Python Model** (incorrect - different training):
- Files: `pyclnf/pyclnf/models/exported_pdm/`
  - `mean_shape.npy` (204,1) - ALREADY FIXED to match C++
  - `eigen_values.npy` (1,34) - DIFFERENT
  - `princ_comp.npy` (204,34) - DIFFERENT (different shape: 204√ó34 vs 204√ó30)

The Python model appears to be from a different export or training run, NOT from the OpenFace C++ model.

#### Root Cause of Persistent Asymmetry

The asymmetry persists because:
1. ‚úÖ Mean shape was fixed to match C++
2. ‚ùå Eigenvalues are completely different (34 vs 30 modes, different values)
3. ‚ùå Eigenvectors are completely different (different PCA decomposition)

**The PDM regularization uses `reg_factor / eigenvalues`**, so different eigenvalues mean different per-mode constraints. This causes different shape deformations even from identical initialization.

#### Recommended Fix

**Export and replace ALL PDM components from C++ to Python:**

1. **mean_shape.npy** - ‚úÖ Already done
2. **eigen_values.npy** - Export 30 values from `pdm_68_aligned_menpo.txt` line 421
3. **princ_comp.npy** - Export 204√ó30 matrix from `pdm_68_aligned_menpo.txt` lines 213-416

**Export Script (to be run):**
```python
import numpy as np

cpp_pdm_file = "/Users/johnwilsoniv/repo/fea_tool/external_libs/openFace/OpenFace/lib/local/LandmarkDetector/model/pdms/pdm_68_aligned_menpo.txt"

# Read all values
with open(cpp_pdm_file) as f:
    content = f.read()

# Parse eigenvalues from line 421 (after "# The variances of the components")
# Format: 30 space-separated values
eigenvalues_str = "1260.970668 622.171792 514.701969 368.231621 178.288803 141.373501 123.600203 108.077802 95.536656 70.169455 60.351993 56.235863 49.519216 44.933492 42.272445 39.564849 34.747298 28.830661 26.147964 25.285930 21.833110 18.995127 18.134667 16.329230 13.965430 12.933174 11.705294 11.301406 9.624030 8.423463"
cpp_eigenvalues = np.array([float(x) for x in eigenvalues_str.split()], dtype=np.float32)
cpp_eigenvalues = cpp_eigenvalues.reshape(1, 30)

# Save
np.save("pyclnf/pyclnf/models/exported_pdm/eigen_values.npy", cpp_eigenvalues)
print(f"Saved eigenvalues shape: {cpp_eigenvalues.shape}")
```

#### Next Steps

1. Export correct eigenvalues from C++ (30 values, not 34)
2. Export correct eigenvectors from C++ (204√ó30 matrix)
3. Update PDM loader to handle 30 modes instead of 34
4. Re-test landmark accuracy on IMG_0422

#### Files to Modify

| File | Current | Target |
|------|---------|--------|
| `pyclnf/pyclnf/models/exported_pdm/eigen_values.npy` | (1, 34) | (1, 30) |
| `pyclnf/pyclnf/models/exported_pdm/princ_comp.npy` | (204, 34) | (204, 30) |
| `pyclnf/pyclnf/core/pdm.py` | Expects 34 modes | Update for 30 modes |

---

### 2024-12-05: PDM Export and Test Results

#### PDM Components Exported Successfully

All 3 PDM components now match C++ OpenFace exactly:

| Component | Old Shape | New Shape | Source |
|-----------|-----------|-----------|--------|
| mean_shape.npy | (204, 1) | (204, 1) | Already correct |
| eigen_values.npy | (1, 34) | **(1, 30)** | Exported from C++ |
| princ_comp.npy | (204, 34) | **(204, 30)** | Exported from C++ |

Old 34-mode model backed up to: `pyclnf/pyclnf/models/exported_pdm/backup_34mode/`

#### Test Results on IMG_0422 (bearded video)

| Metric | Before PDM Fix | After PDM Fix | Change |
|--------|----------------|---------------|--------|
| LEFT jaw error | 16.11 px | **10.63 px** | -34% ‚úì |
| RIGHT jaw error | 6.45 px | **5.13 px** | -20% ‚úì |
| LEFT/RIGHT ratio | 2.50x | **2.07x** | Improved ‚úì |
| Overall mean error | ~11.5 px | **3.60 px** | -69% ‚úì |

#### Analysis: Asymmetry Still Present

Despite matching the C++ PDM model exactly, the left/right asymmetry persists (2.07x vs 2.50x). This confirms the asymmetry is NOT caused by the PDM model differences.

**Confirmed NOT the cause:**
- ‚úÖ Mean shape (now matches C++ exactly)
- ‚úÖ Eigenvalues (now matches C++ exactly - 30 modes, same values)
- ‚úÖ Eigenvectors (now matches C++ exactly - 204√ó30)
- ‚úÖ MirroredCENPatchExpert (verified correct flip-process-flip)
- ‚úÖ Mirror indices (verified bidirectional)

**Remaining suspects:**
1. **Patch expert neural network weights** - The CEN weights may differ from C++
2. **Response map extraction** - How areas of interest are extracted may differ
3. **Mean-shift kernel/sigma** - Convergence behavior may differ
4. **Temporal tracking** - Video mode state accumulation may differ

#### Per-Region Errors (After PDM Fix)

| Region | Mean Error (px) | Status |
|--------|-----------------|--------|
| Jaw (0-16) | 8.04 | Highest - beard occlusion |
| Eyebrow (17-26) | 1.51 | Good |
| Nose (27-35) | 2.85 | Moderate |
| Eye (36-47) | 1.51 | Good |
| Mouth (48-67) | 2.46 | Moderate |

The jaw region remains the primary error source, consistent with beard occlusion.

---

### 2025-12-05: CEN Patch Expert Paradox Discovery

#### Context
After fixing the PDM model (30 modes, matching C++ exactly), the left/right asymmetry persisted at 2.07x. Direct comparison of Python vs C++ landmarks on IMG_0942.MOV frame 10 revealed a surprising paradox.

#### Critical Finding: REAL Experts Perform WORSE Than Mirrored

| Jaw Side | Landmarks | Expert Type | Mean Error vs C++ |
|----------|-----------|-------------|-------------------|
| **LEFT** | 0-7 | REAL (direct weights) | **4.62 px** |
| **RIGHT** | 9-16 | MIRRORED (flip-process-flip) | **2.31 px** |
| **Ratio** | - | - | **2.00x** |

**This is paradoxical**: Landmarks with REAL trained weights have 2x HIGHER error than landmarks using mirrored experts (flip-process-flip approach).

#### Verified Correct Implementations

1. **MirroredCENPatchExpert**: Produces identical results to manual flip-process-flip (max diff: 0.0)
2. **Mirror indices**: Correct bidirectional mapping (0‚Üî16, 1‚Üî15, etc.)
3. **Empty flags**: LM 0-7 have `is_empty=False`, LM 9-16 have `is_empty=True`

Test output confirming mirroring is correct:
```
Method 1 (MirroredCENPatchExpert):
  Response shape: (11, 11)
  Response stats: min=0.006206, max=0.030220

Method 2 (Manual flip-process-flip):
  Response shape: (11, 11)
  Response stats: min=0.006206, max=0.030220

Comparison:
  Max difference: 0.0
  Responses match: True
```

#### Hypotheses for the Paradox

1. **CEN weights trained for flipped images**: The neural network may have been trained on images with a specific orientation. The flip-process-flip approach may "correct" an orientation mismatch.

2. **Patch extraction coordinates differ from C++**: The area-of-interest extraction for LEFT landmarks may compute coordinates differently than C++, while RIGHT landmarks benefit from the mirroring which "normalizes" the coordinates.

3. **Response map interpretation**: C++ may use `ResponseSparse()` which processes left/right pairs together, flipping the RIGHT AOI before im2col. Python processes them separately, which could cause subtle differences.

4. **OpenFace training data bias**: The OpenFace documentation notes that training data biases lead to non-symmetric results. The flip-process-flip may partially compensate for these biases.

#### C++ ResponseSparse() Analysis

From `CEN_patch_expert.cpp:546-616`:
```cpp
void CEN_patch_expert::ResponseSparse(Mat_<float>& response_left, Mat_<float>& response_right,
    const Mat_<float>& area_of_interest_left, const Mat_<float>& area_of_interest_right)
{
    // Process LEFT AOI directly
    im2col_left = im2col_t(area_of_interest_left);

    // Process RIGHT AOI: flip BEFORE im2col
    cv::flip(area_of_interest_right, aoi_right_flipped, 1);
    im2col_right = im2col_t(aoi_right_flipped);

    // Process both through same neural network together
    // ...

    // Flip RIGHT response back
    cv::flip(response_right, response_right, 1);
}
```

Both Python and C++ flip the RIGHT AOI before processing and flip the response back. The implementations match.

#### Next Steps to Investigate

1. **Compare patch AOI coordinates**: Log the exact pixel coordinates of the areas of interest for LEFT vs RIGHT landmarks in both Python and C++

2. **Compare response map values**: Export response maps from both Python and C++ for the same frame and compare numerically

3. **Test with flipped input image**: Process a horizontally flipped image and check if the asymmetry reverses (would confirm orientation bias)

4. **Check im2col implementation**: Ensure Python's im2col produces identical column matrices to C++

#### Conclusion

The paradox suggests that the LEFT side processing has a systematic error that the RIGHT side avoids through the flip-process-flip approach. This could be:
- An AOI extraction difference that gets "corrected" by flipping
- A neural network weight orientation issue
- A subtle numerical precision difference in direct vs mirrored processing

Further investigation needed to determine the exact cause.

---

### 2025-12-05: Flip Test Results - Critical Finding

#### Test Design
Process the same frame both normally and horizontally flipped, then compare errors.

If the asymmetry were due to:
- **CEN weight orientation bias**: Asymmetry would REVERSE (right side would become worse in flipped version)
- **Python processing code issue**: Asymmetry would PERSIST (left landmarks always worse regardless of face content)

#### Results

| Condition | LEFT Jaw Error | RIGHT Jaw Error | Ratio |
|-----------|----------------|-----------------|-------|
| **Normal frame** | 4.60 px | 2.21 px | **2.08x** |
| **Flipped frame** | 4.67 px | 2.05 px | **2.28x** |

**The asymmetry did NOT reverse!**

#### Conclusion

The asymmetry is **NOT caused by CEN weight orientation bias**. Instead, it's caused by something in the **Python processing code** that treats landmarks 0-7 (which use REAL experts) differently from landmarks 9-16 (which use MIRRORED experts).

Since:
1. MirroredCENPatchExpert produces identical output to manual flip-process-flip ‚úÖ
2. Flip test shows asymmetry persists regardless of image orientation ‚úÖ
3. LEFT landmarks (with REAL experts) always have higher error ‚úÖ

The issue must be in **how the Python code processes paired landmarks**. Specifically, the `response_sparse()` function that handles left/right pairs together may have a subtle difference from C++.

#### Next Investigation: response_sparse() implementation

The `response_sparse()` function in `cen_patch_expert.py:217-328`:
1. Takes left and right AOIs
2. Flips RIGHT AOI horizontally before im2col
3. Processes both through the neural network
4. Flips RIGHT response back

Key areas to check:
1. Is the im2col implementation identical for left vs right?
2. Is there any asymmetry in how the response is computed?
3. Is the response map interpretation (argmax location) handled identically?

---

### 2025-12-05: Deep Investigation Results

#### Test 1: response_sparse() vs Individual Processing

Tested whether batched `response_sparse()` produces different results than individual processing:

```
=== Method 1: response_sparse (batched) ===
Left response:  shape=(11, 11), max=0.042280
Right response: shape=(11, 11), max=0.028550

=== Method 2: Individual processing ===
Left (individual): shape=(11, 11), max=0.042280
Right (flip-process-flip): shape=(11, 11), max=0.028550

=== Comparison ===
LEFT:  batched vs individual - max_diff=0.0000000000
RIGHT: batched vs individual - max_diff=0.0000000000

>>> Batched and individual processing produce IDENTICAL results.
```

**Conclusion:** The batching in `response_sparse()` is NOT the cause of the asymmetry.

#### Test 2: Mean-shift and AOI Extraction Code Review

Reviewed `_compute_mean_shift()` and `_extract_area_of_interest()` - both apply **identical** processing to all landmarks regardless of index. No special handling for left (0-7) vs right (9-16).

#### Summary of What's Been Ruled Out

| Component | Status | Notes |
|-----------|--------|-------|
| CEN weight orientation bias | ‚úÖ Ruled out | Flip test shows asymmetry persists |
| MirroredCENPatchExpert | ‚úÖ Correct | Matches manual flip-process-flip |
| response_sparse() batching | ‚úÖ Correct | Identical to individual processing |
| Mirror indices | ‚úÖ Correct | Bidirectional 0‚Üî16, 1‚Üî15, etc. |
| Mean-shift computation | ‚úÖ Symmetric | Same code for all landmarks |
| AOI extraction | ‚úÖ Symmetric | Same code for all landmarks |

#### Remaining Hypothesis: PDM Regularization During Optimization

The asymmetry could be coming from:

1. **PDM shape constraints** - The regularization term in the optimizer may affect different shape parameters differently
2. **Optimization convergence** - Left vs right landmarks may converge at different rates due to eigenvalue weighting
3. **C++ also has this asymmetry** - OpenFace documentation notes training data biases cause non-symmetric results

#### Current State

The Python implementation appears to be **correct** but produces asymmetric results. This may be inherent to the OpenFace model design. Further investigation would require:

1. Comparing intermediate optimization values (params_local) between Python and C++
2. Checking if C++ OpenFace produces the same left/right asymmetry pattern
3. Comparing regularization matrix application during Jacobian computation

---

### 2024-12-06: pyMTCNN Exact Match Achieved + pyCLNF Cleanup

#### pyMTCNN: Production Ready with Exact C++ Match

**Achievement:** pyMTCNN now achieves **< 0.001 px error** vs C++ OpenFace MTCNN implementation.

**Key Changes:**
1. Refactored backends (CoreML: 693‚Üí130 lines, ONNX: 514‚Üí169 lines)
2. Fixed model paths and output key discovery
3. Bbox format verified: [x, y, width, height] not [x1, y1, x2, y2]

**Test Results (test_face_clean.png):**
```
Bbox error: 0.0002 px
Landmark error: 0.0003 px
```

Both bounding boxes and 5-point landmarks match C++ implementation exactly.

#### pyCLNF: MTCNN Bbox Correction Applied

**Critical Discovery:** C++ OpenFace applies a bbox correction AFTER MTCNN detection:
```cpp
// From FaceDetectorMTCNN.cpp lines 1496-1499
x = w * -0.0075 + x;
y = h * 0.2459 + y;  // ~157px y-offset for typical face
w = 1.0323 * w;
h = 0.7751 * h;      // ~144px height reduction
```

**Implementation:** Added `_apply_mtcnn_bbox_preprocessing()` to `pdm.py`:
```python
def _apply_mtcnn_bbox_preprocessing(self, bbox):
    x, y, w, h = bbox
    corrected_x = w * -0.0075 + x
    corrected_y = h * 0.2459 + y
    corrected_w = 1.0323 * w
    corrected_h = 0.7751 * h
    return (corrected_x, corrected_y, corrected_w, corrected_h)
```

**Result:** Landmark error improved from **61.94 px ‚Üí 1.34 px** (46x improvement!)

#### pyCLNF: Multi-Hypothesis Rotation Testing Implemented

Added `fit_multi_hypothesis()` method matching C++ `DetectLandmarksInImageMultiHypBasic`:

**11 rotation hypotheses tested:**
```python
rotation_hypotheses = [
    (0, 0, 0),           # frontal
    (0, -0.5236, 0),     # yaw -30¬∞
    (0, 0.5236, 0),      # yaw +30¬∞
    (0, -0.96, 0),       # yaw -55¬∞
    (0, 0.96, 0),        # yaw +55¬∞
    (0, 0, 0.5236),      # roll +30¬∞
    (0, 0, -0.5236),     # roll -30¬∞
    (0, -1.57, 0),       # yaw -90¬∞ (profile left)
    (0, 1.57, 0),        # yaw +90¬∞ (profile right)
    (0, -1.22, 0.698),   # combined
    (0, 1.22, -0.698),   # combined
]
```

**Test Results:**
| Mode | Mean Error | Max Error |
|------|------------|-----------|
| Multi-hypothesis | 2.35 px | 7.63 px |
| Single (frontal) | 2.24 px | 14.16 px |

Multi-hypothesis reduces max error but slightly increases mean error. Both converge to similar final rotations.

#### Code Cleanup Completed

**Debug logging removed from:**
- `clnf.py` - Removed init landmarks debug file writes
- `optimizer.py` - Removed param update debug file writes (-92 lines)
- `eye_patch_expert.py` - Removed all 11 debug blocks (-256 lines)

**Net result:** -369 lines of debug code removed

#### Current State

**pyCLNF achieves 1.34-2.24 px mean error** vs C++, with errors concentrated on:
- Inner mouth landmarks (56, 65)
- Jawline landmarks (3, 4, 5)

**Next Investigation Needed:**
1. Compare initialization params (scale, rotation, translation) at frame start
2. Compare iteration-by-iteration CLNF fitting to find divergence point
3. Investigate MTCNN 5-point landmark initialization differences

---

### 2024-12-06: Systematic Per-Iteration Investigation Plan

#### Goal
Achieve exact match to C++ (< 0.001 px) for pyCLNF, like we did for pyMTCNN.

#### Investigation Steps

**Step 1: Initialization Comparison**
- Compare `params_global` after init: [scale, rot_x, rot_y, rot_z, tx, ty]
- Compare `params_local` (shape parameters)
- Verify MTCNN bbox correction is applied identically

**Step 2: Per-Iteration Comparison**
- Add C++ debug output for each iteration
- Compare mean-shift vectors
- Compare Jacobian computation
- Compare parameter updates

**Step 3: MTCNN 5-Point Landmark Integration**
- C++ can use 5-point landmarks for initial pose estimation
- Verify Python implements same approach
- Compare rotation estimation from landmarks

#### Key Files to Compare

| Component | Python File | C++ File |
|-----------|-------------|----------|
| Initialization | `pdm.py:init_params()` | `PDM.cpp:CalcParams()` |
| Mean-shift | `optimizer.py:_compute_mean_shift()` | `LandmarkDetectorModel.cpp:GetResponsePixels()` |
| Param update | `optimizer.py:_solve_update()` | `LandmarkDetectorModel.cpp:NU_RLMS()` |
| Jacobian | `pdm.py:compute_jacobian()` | `PDM.cpp:ComputeJacobian()` |

---

### 2024-12-06: Iteration-by-Iteration Comparison Results

#### Test Image: test_face_clean.png

#### Finding 1: Initialization MATCHES Exactly ‚úÖ

Python and C++ produce **identical** initial parameters:

```
Python init:  scale=3.481482, rot=(0, 0, 0), tx=543.718, ty=940.178
C++ init:     scale=3.48148,  rot=(0, 0, 0), tx=543.718, ty=940.178
```

**Conclusion:** Initialization is NOT the source of divergence.

#### Finding 2: Jawline Landmarks Have Large Y-Axis Errors

| Region | Mean Error | Max Error | Pattern |
|--------|------------|-----------|---------|
| Jawline (0-16) | 4.54 px | 14.16 px | Python ABOVE C++ |
| Left eyebrow | 1.12 px | 1.87 px | - |
| Right eyebrow | 0.90 px | 1.11 px | - |
| Nose | 1.01 px | 1.83 px | - |
| Left eye | 0.52 px | 0.99 px | - |
| Right eye | 0.92 px | 1.60 px | - |
| Outer mouth | 2.67 px | 4.00 px | - |

**Worst landmarks (all left jawline):**
- LM4: 14.16 px error (Python Y=1090 vs C++ Y=1104)
- LM3: 12.80 px error
- LM5: 11.71 px error
- LM2: 9.56 px error

**Pattern:** Python jawline landmarks are **shifted UPWARD** (lower Y values) compared to C++.

#### Finding 3: Shape Parameters (params_local) Diverge Significantly

```
C++ Final params_local (from iter0 debug - not final):
  mode0: 12.80, mode1: -5.73, mode2: -9.22, mode3: -4.36, mode4: -30.78

Python Final params_local:
  mode0: 25.85, mode1: -5.56, mode2: -11.01, mode3: +4.56, mode4: -31.04

DIFFERENCES:
  mode0: +13.05 (HUGE - controls face height!)
  mode3: +8.92 (significant - changes jawline shape)
```

**Root Cause:** Shape parameters diverge during optimization, causing the jawline to appear shorter/higher in Python.

#### Finding 4: C++ Uses 4 Window Sizes, Python Uses 3

```
C++ window sizes: [11, 9, 7, 5] - includes finest scale
Python window sizes: [11, 9, 7] - missing WS5 (no sigma_components)
```

**Note:** Window size 5 was intentionally excluded from Python because:
1. No sigma_components file exists for WS5
2. Testing showed WS5 WITHOUT sigma degrades accuracy

However, C++ does use WS5 and achieves better jawline convergence.

#### Finding 5: C++ Iteration Count Differences

```
C++ iterations per phase (estimated):
  RIGID: ~1-2 iterations (fast convergence)
  NONRIGID: ~1-2 iterations (fast convergence)

Python iterations per phase:
  RIGID: 10 iterations (max)
  NONRIGID: 10 iterations (converging slowly)
```

**Observation:** Python runs more iterations but converges to different local params.

#### Hypothesis: Mean-Shift or Jacobian Computation Difference

The divergence happens during optimization, not initialization. Possible causes:

1. **Response map differences** - Slight differences in patch expert responses
2. **Mean-shift computation** - KDE kernel or offset transformation
3. **Regularization application** - Lambda_inv matrix computation
4. **Jacobian computation** - How derivatives are computed

#### Next Steps

1. [x] Compare mean-shift vectors for identical response maps - DONE
2. [ ] Compare Jacobian matrices element-by-element
3. [ ] Add C++ debug output for params_local at each iteration
4. [ ] Investigate sigma_components for window size 5

---

### 2024-12-06: Root Cause Found - Sigma Transform Difference

#### Key Finding: Response Maps Differ at ~5% Level

Comparing LM36 response maps (after sigma transform):

| Metric | Python | C++ | Diff |
|--------|--------|-----|------|
| min | 0.00744 | 0.00744 | 0% |
| max | 0.8217 | 0.8248 | -0.4% |
| mean | 0.0535 | 0.0523 | +2.3% |
| KDE-weighted sum | 3.010 | 2.852 | +5.6% |

#### Critical Position Difference

At position (7,5) - one of the peak response locations:

```
Python BEFORE sigma: 0.5386
Python AFTER sigma:  0.8209  (1.52x increase)
C++ AFTER sigma:     0.4978

DIFFERENCE: 0.323 (65% higher in Python!)
```

**Root Cause:** The sigma transform is applying differently:
- Python: Increases value from 0.54 ‚Üí 0.82 (1.52x)
- C++: Keeps value at 0.50 (or even reduces it)

This difference in ONE position causes the mean-shift to be pulled in a different direction, which accumulates over multiple iterations.

#### Mean-Shift Impact

For LM36 (iter0, ws11):
```
                        C++             Python          Diff
msx (mean-shift x):    -0.2420         -0.2859         -0.044
msy (mean-shift y):     1.8796          1.9498         +0.070

In image coords (√ó13.9 scale):
                        C++             Python          Diff
ms_img_x:              -3.37           -3.98           -0.61 px
ms_img_y:              26.18           27.15           +0.97 px
```

This ~1px difference per iteration accumulates to 10-14px error in jawline landmarks over 40+ total iterations.

#### Investigation Needed

The sigma transform in `optimizer.py:_compute_response_map()` calls:
```python
Sigma = patch_expert.compute_sigma(sigma_comps, window_size=response_window_size)
response_map = (Sigma @ response_vec).reshape(response_shape)
```

Need to compare:
1. `sigma_comps` loading from model files
2. `compute_sigma()` matrix computation
3. Matrix multiplication application

#### Temporary Workaround Tested

Disabling sigma transform entirely:
- Makes Python response == C++ response (BEFORE sigma)
- But C++ also applies sigma, so this doesn't fix the root cause

The issue is that Python and C++ sigma transforms produce DIFFERENT results from the SAME input.
